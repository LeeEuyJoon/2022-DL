{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RNN으로 BBC기사 분류하기 (자연어 처리)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 수입\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk     # Natural Language Toolkit\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding, SimpleRNN\n",
    "from keras.layers import Bidirectional # 쌍방향 RNN 기술 : 미래에서 과거 방향 추가\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "MY_WORDS  = 5000    # 사전의 단어 개수\n",
    "MY_EMBED  = 64      # 임베딩 차원 / 차원 줄인 후 결과 5000 > 64\n",
    "MY_HIDDEN = 100     # LSTM의 차원, 현재 단에서 다음 단으로 넘겨주는 데이터의 개수\n",
    "MY_LEN = 200        # 통일된 기사 길이\n",
    "\n",
    "MY_SPLIT = 0.8      # TRAIN, TEST 비율\n",
    "MY_EPOCH = 10       # 반복 학습 수\n",
    "\n",
    "# 실행 모드 선택\n",
    "TRAIN_MODE = 1      # 학습 모드인지 (1) 평가 모드인지 선택 (0) / 평가 모드 시, 학습을 종료한 가중치를 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제외어\n",
      "{'i', 'do', 'yours', 'these', 'above', 've', 'is', 'before', 'don', \"that'll\", \"you've\", 'or', 'when', 'myself', 'as', 'over', 'whom', 'to', \"hadn't\", 'm', 't', 'shan', 'because', 'with', 'his', 'same', 'were', 'where', 'what', 'has', 'up', 'ourselves', 'it', 'such', 'who', \"shouldn't\", 'how', \"won't\", \"isn't\", 'of', 'than', 'didn', \"didn't\", 'haven', \"should've\", 'weren', 'o', 'on', 'doesn', 'her', 'more', 'between', 'our', 'no', 'from', 'which', 'then', 'your', 'them', 'just', 'won', 'ours', \"couldn't\", 'some', 'below', 'he', 'but', 'at', 'out', 'further', 'having', 'while', 'few', 'not', 'during', 'will', 'y', 'ma', 'for', 'itself', 'other', 'this', 'that', 'be', 'under', \"you'll\", 'off', 'those', 'their', 'isn', 'themselves', 'hasn', 'down', 'yourselves', \"weren't\", 's', 'does', \"hasn't\", \"haven't\", 'again', 'the', 'are', \"she's\", 'mightn', 'against', 'can', 'wasn', 'both', 'was', 'they', 'am', 'all', 'hers', 'a', 'll', 'now', 'so', 'herself', 'into', 'him', 'himself', 'needn', 'most', 'its', \"you'd\", 'nor', \"don't\", \"mustn't\", 'here', 'very', 'an', 'had', \"shan't\", 'there', 'until', 'too', 'did', \"doesn't\", \"wasn't\", 'once', 'wouldn', \"wouldn't\", 'being', 'yourself', 're', 'theirs', 'couldn', 'been', 'only', 'through', 'she', 'we', 'shouldn', 'any', 'own', 'my', 'why', 'about', \"mightn't\", 'have', \"aren't\", 'if', 'after', 'hadn', \"it's\", 'by', 'mustn', 'doing', \"you're\", 'and', 'aren', 'd', 'each', 'should', 'ain', 'in', 'you', \"needn't\", 'me'}\n",
      "제외어 개수: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wds66\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 제외어 처리 : 흔해 빠진 단어들, 분류에 도움이 안되는 단어들\n",
    "# 데이터의 부피 감소 = 학습 시간 개선\n",
    "nltk.download('stopwords')\n",
    "MY_STOP = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "print('제외어')\n",
    "print(MY_STOP)\n",
    "print('제외어 개수:',len(MY_STOP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리한 기사 수: 2225\n"
     ]
    }
   ],
   "source": [
    "# 데이터 저장 창고\n",
    "original = []   # 기사 원본\n",
    "finished = []   # 기사 처리본\n",
    "labels = []     # 기사 카테고리\n",
    "\n",
    "# 데이터 읽기\n",
    "path = 'C:/Users/wds66/Desktop/work/2022_Georgia/bbc-text.csv'\n",
    "\n",
    "with open(path,'r') as file:\n",
    "    # 컬럼 헤딩 처리\n",
    "    finger = csv.reader(file)\n",
    "    header = next(finger)\n",
    "    # print(header)\n",
    "    \n",
    "    # 기사 하나씩 처리\n",
    "    for row in finger:\n",
    "        # print(row)\n",
    "        labels.append(row[0])\n",
    "        original.append(row[1])\n",
    "        news = row[1]\n",
    "        \n",
    "        # 제외어 처리\n",
    "        for word in MY_STOP:\n",
    "            mask = ' ' + word + ' '\n",
    "            news = news.replace(mask,' ')\n",
    "        # print(news)\n",
    "        finished.append(news)\n",
    "\n",
    "print('처리한 기사 수:',len(finished))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 기사 원본: screensaver tackles spam websites net users are getting the chance to fight back against spam websites  internet portal lycos has made a screensaver that endlessly requests data from sites that sell the goods and services mentioned in spam e-mail. lycos hopes it will make the monthly bandwidth bills of spammers soar by keeping their servers running flat out. the net firm estimates that if enough people sign up and download the tool  spammers could end up paying to send out terabytes of data.   we ve never really solved the big problem of spam which is that its so damn cheap and easy to do   said malte pollmann  spokesman for lycos europe.  in the past we have built up the spam filtering systems for our users   he said   but now we are going to go one step further.    we ve found a way to make it much higher cost for spammers by putting a load on their servers.  by getting thousands of people to download and use the screensaver  lycos hopes to get spamming websites constantly running at almost full capacity. mr pollmann said there was no intention to stop the spam websites working by subjecting them with too much data to cope with. he said the screensaver had been carefully written to ensure that the amount of traffic it generated from each user did not overload the web.  every single user will contribute three to four megabytes per day   he said   about one mp3 file.  but  he said  if enough people sign up spamming websites could be force to pay for gigabytes of traffic every single day. lycos did not want to use e-mail to fight back  said mr pollmann.  that would be fighting one bad thing with another bad thing   he said.  the sites being targeted are those mentioned in spam e-mail messages and which sell the goods and services on offer.  typically these sites are different to those that used to send out spam e-mail and they typically only get a few thousand visitors per day. the list of sites that the screensaver will target is taken from real-time blacklists generated by organisations such as spamcop. to limit the chance of mistakes being made  lycos is using people to ensure that the sites are selling spam goods. as these sites rarely use advertising to offset hosting costs  the burden of high-bandwidth bills could make spam too expensive  said mr pollmann. sites will also slow down under the weight of data requests. early results show that response times of some sites have deteriorated by up to 85%. users do not have to be registered users of lycos to download and use the screensaver. while working  the screensaver shows the websites that are being bothered with requests for data. the screensaver is due to be launched across europe on 1 december and before now has only been trialled in sweden. despite the soft launch  mr pollmann said that the screensaver had been downloaded more than 20 000 times in the last four days.  there s a huge user demand to not only filter spam day-by-day but to do something more   he said  before now users have never had the chance to be a bit more offensive.\n",
      "글자 수 : 3046\n",
      "단어 수 : 536\n",
      "샘플 기사 제외어 처리본 : screensaver tackles spam websites net users getting chance fight back spam websites  internet portal lycos made screensaver endlessly requests data sites sell goods services mentioned spam e-mail. lycos hopes make monthly bandwidth bills spammers soar keeping servers running flat out. net firm estimates enough people sign download tool  spammers could end paying send terabytes data.   never really solved big problem spam damn cheap easy   said malte pollmann  spokesman lycos europe.  past built spam filtering systems users   said   going go one step further.    found way make much higher cost spammers putting load servers.  getting thousands people download use screensaver  lycos hopes get spamming websites constantly running almost full capacity. mr pollmann said intention stop spam websites working subjecting much data cope with. said screensaver carefully written ensure amount traffic generated user overload web.  every single user contribute three four megabytes per day   said   one mp3 file.   said  enough people sign spamming websites could force pay gigabytes traffic every single day. lycos want use e-mail fight back  said mr pollmann.  would fighting one bad thing another bad thing   said.  sites targeted mentioned spam e-mail messages sell goods services offer.  typically sites different used send spam e-mail typically get thousand visitors per day. list sites screensaver target taken real-time blacklists generated organisations spamcop. limit chance mistakes made  lycos using people ensure sites selling spam goods. sites rarely use advertising offset hosting costs  burden high-bandwidth bills could make spam expensive  said mr pollmann. sites also slow weight data requests. early results show response times sites deteriorated 85%. users registered users lycos download use screensaver. working  screensaver shows websites bothered requests data. screensaver due launched across europe 1 december trialled sweden. despite soft launch  mr pollmann said screensaver downloaded 20 000 times last four days.  huge user demand filter spam day-by-day something   said  users never chance bit offensive.\n",
      "제외어 처리 하고 단어 수: 303\n"
     ]
    }
   ],
   "source": [
    "# 샘플 출력\n",
    "print('샘플 기사 원본:',original[123])\n",
    "print('글자 수 :',len(original[123]))\n",
    "print('단어 수 :',len(original[123].split()))\n",
    "\n",
    "print('샘플 기사 제외어 처리본 :',finished[123])\n",
    "print('제외어 처리 하고 단어 수:',len(finished[123].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 기사에 등장한 고유한 단어 수 : 29698\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 기사 데이터 토큰 처리\n",
    "# 단어를 고유의 정수로 전환\n",
    "# OOV: out-of-vocabulary\n",
    "A_token = Tokenizer(num_words=MY_WORDS,\n",
    "                    oov_token='!')\n",
    "A_token.fit_on_texts(finished)\n",
    "\n",
    "# 단어 토큰화 (num_words 미적용)\n",
    "# print(A_token.word_counts)\n",
    "print('전체 기사에 등장한 고유한 단어 수 :',len(A_token.word_counts))\n",
    "# print('토큰화 결과 :',A_token.word_index)\n",
    "\n",
    "# 기사 토큰화 (num_words 적용)\n",
    "finished = A_token.texts_to_sequences(finished)\n",
    "print(type(finished))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사: 0 , 길이: 425\n",
      "기사: 1 , 길이: 192\n",
      "기사: 2 , 길이: 132\n",
      "기사: 3 , 길이: 275\n",
      "기사: 4 , 길이: 188\n",
      "기사: 5 , 길이: 355\n",
      "기사: 6 , 길이: 153\n",
      "기사: 7 , 길이: 113\n",
      "기사: 8 , 길이: 102\n",
      "기사: 9 , 길이: 136\n",
      "제일 긴 기사의 길이: 2279\n",
      "제일 짧은 기사의 길이: 50\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 기사 샘플\n",
    "# print('원본 :',original[123])\n",
    "# print('토큰화 처리본:',finished[123])\n",
    "\n",
    "# 기사 길이 통계\n",
    "for i in range(10):\n",
    "    print('기사:',i,', 길이:',len(finished[i]))    \n",
    "longest = max([len(x) for x in finished])\n",
    "print('제일 긴 기사의 길이:',longest)\n",
    "shortest = min([len(x) for x in finished])\n",
    "print('제일 짧은 기사의 길이:',shortest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사: 0 , 길이: 200\n",
      "기사: 1 , 길이: 200\n",
      "기사: 2 , 길이: 200\n",
      "기사: 3 , 길이: 200\n",
      "기사: 4 , 길이: 200\n",
      "기사: 5 , 길이: 200\n",
      "기사: 6 , 길이: 200\n",
      "기사: 7 , 길이: 200\n",
      "기사: 8 , 길이: 200\n",
      "기사: 9 , 길이: 200\n",
      "제일 긴 기사의 길이: 200\n",
      "제일 짧은 기사의 길이: 200\n"
     ]
    }
   ],
   "source": [
    "# 기사 길이 통일\n",
    "finished = pad_sequences(finished,\n",
    "                            truncating='pre',\n",
    "                            padding = 'pre',\n",
    "                            maxlen=MY_LEN)\n",
    "\n",
    "# 기사 길이 통계\n",
    "for i in range(10):\n",
    "    print('기사:',i,', 길이:',len(finished[i]))    \n",
    "longest = max([len(x) for x in finished])\n",
    "print('제일 긴 기사의 길이:',longest)\n",
    "shortest = min([len(x) for x in finished])\n",
    "print('제일 짧은 기사의 길이:',shortest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨 단어의 빈도수: OrderedDict([('tech', 401), ('business', 510), ('sport', 511), ('entertainment', 386), ('politics', 417)])\n",
      "토큰화 결과 {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n"
     ]
    }
   ],
   "source": [
    "# 라벨 토큰화\n",
    "C_token = Tokenizer()\n",
    "C_token.fit_on_texts(labels)\n",
    "\n",
    "# 단어를 토큰화\n",
    "print('라벨 단어의 빈도수:',C_token.word_counts)\n",
    "print('토큰화 결과',C_token.word_index)\n",
    "\n",
    "# 전체 기사 카테고리 토큰화\n",
    "labels = C_token.texts_to_sequences(labels)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "X_train 모양 : (1780, 200)\n",
      "Y_train 모양 : (1780, 1)\n",
      "X_test 모양  : (445, 200)\n",
      "Y_test 모양  : (445, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 4분할\n",
    "print(type(finished))\n",
    "print(type(labels))\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(finished,\n",
    "                                                    labels,\n",
    "                                                    train_size=MY_SPLIT,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "print('X_train 모양 :',X_train.shape)\n",
    "print('Y_train 모양 :',Y_train .shape)\n",
    "print('X_test 모양  :',X_test.shape)\n",
    "print('Y_test 모양  :',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               66000     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 386,606\n",
      "Trainable params: 386,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 기사 분류기 구현\n",
    "model = Sequential()\n",
    "\n",
    "# 임베딩 층 추가\n",
    "model.add(Embedding(input_dim=MY_WORDS,\n",
    "                    output_dim=MY_EMBED,\n",
    "                    input_length=MY_LEN))\n",
    "\n",
    "# LSTM 추가\n",
    "model.add(LSTM(units=MY_HIDDEN,\n",
    "                input_shape=(MY_LEN,MY_EMBED)))\n",
    "\n",
    "# 출력층\n",
    "model.add(Dense(units=6,\n",
    "            activation='softmax'))\n",
    "\n",
    "# RNN 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n",
      "Epoch 1/10\n",
      "56/56 [==============================] - 7s 96ms/step - loss: 1.6481 - acc: 0.2489\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 6s 100ms/step - loss: 1.3239 - acc: 0.4404\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 6s 102ms/step - loss: 0.9011 - acc: 0.7101 1s - loss: \n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 6s 104ms/step - loss: 0.4700 - acc: 0.8876\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 6s 105ms/step - loss: 0.1578 - acc: 0.9663\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 6s 106ms/step - loss: 0.0843 - acc: 0.9775\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 6s 105ms/step - loss: 0.0301 - acc: 0.9938\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 6s 107ms/step - loss: 0.0235 - acc: 0.9955\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 6s 112ms/step - loss: 0.0256 - acc: 0.9966\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 6s 113ms/step - loss: 0.0204 - acc: 0.9972\n",
      "총 학습 시간: 59.99891138076782\n"
     ]
    }
   ],
   "source": [
    "# 13번 셀\n",
    "\n",
    "# RNN 학습\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "# crossentropy                      >> 2진 분류 / 원-핫 ㅇㅇ\n",
    "# categorical_crossentropy          >> 3이상    / 원-핫 ㅇㅇ\n",
    "# sparse_categorical_crossentropy   >> 3이상    / 원-핫 ㄴㄴ 근데 crossentropy는 쓰고 싶어\n",
    "\n",
    "print('학습 시작')\n",
    "begin = time()\n",
    "\n",
    "model.fit(X_train,Y_train,\n",
    "            epochs=MY_EPOCH,\n",
    "            verbose=1)\n",
    "\n",
    "end = time()\n",
    "print('총 학습 시간:',end-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 35ms/step - loss: 0.1896 - acc: 0.9438\n",
      "정확도 : 0.9438202381134033\n"
     ]
    }
   ],
   "source": [
    "# RNN 평가\n",
    "score = model.evaluate(X_test,Y_test)\n",
    "print('정확도 :',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가용 0번 문제 예측 : 5\n",
      "평가용 0번 문제 정답 : [5]\n"
     ]
    }
   ],
   "source": [
    "# RNN 예측\n",
    "# 토큰화 결과 :{1 >> 스포츠 / 2 >> 비즈니스 / 3 >> 정치 / 4 >> 기술 / 5 >> 엔터}\n",
    "pred = model.predict(X_test)\n",
    "print('평가용 0번 문제 예측 :',pred[0].argmax())\n",
    "print('평가용 0번 문제 정답 :',Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US President Joe Biden has called for a historic change to Senate rules as he seeks to overhaul the countrys election laws.In an impassioned speech, he said he supported changes that would allow his voting reforms to be passed without the support of opposition Republicans.Misgivings from two senators in his party are hampering his plans, and no Republicans have backed them.Currently, a majority of 60% is needed to pass most legislation in the Senate.And with the upper chamber of Congress split 50-50 between the two parties, Mr Bidens sweeping election bills are almost certain not to pass unless there is a change to that rule.Such a change is unlikely, analysts say, as it would require the support of every Democrat in the Senate as well as the tie-breaking vote of the vice-president.']\n",
      "토큰화 결과: [[9, 194, 3436, 1, 1, 152, 1662, 1347, 4023, 312, 588, 1, 506, 1, 1, 3537, 588, 1, 1071, 1, 49, 781, 602, 1, 1, 746, 1, 2, 1, 3076, 618, 755, 4, 441, 1, 1516, 1741, 588, 2398, 1684, 185, 1071, 290, 1669, 953, 1, 1, 1, 15, 1, 602, 1, 53, 3926, 1, 1, 101, 1671, 1588, 1, 2050, 1113, 598, 308, 1347, 841, 1669, 1131, 2427, 408, 588, 1524, 1, 1403, 602, 1071, 1, 1671, 2404, 1071, 3509, 3139, 1669, 2699, 2413, 412, 412, 1, 1071, 15, 566, 3, 1, 1, 49, 2849, 3926, 343, 1240, 2340, 588, 1524, 1587, 1490, 2427, 1347, 312, 588, 755, 1010, 1, 1347, 312, 2427, 1269, 271, 71, 1, 217, 4, 2900, 1071, 290, 1669, 224, 1468, 602, 1071, 1, 1, 37, 1, 1071, 1336, 2141, 458, 1669, 1071, 1299, 194]]\n",
      "140\n",
      "단어 수: 200\n",
      "예측 결과: 3\n"
     ]
    }
   ],
   "source": [
    "# BBC 기사로 실습\n",
    "news = ['US President Joe Biden has called for a historic change to Senate rules as he seeks to overhaul the countrys election laws.\\\n",
    "In an impassioned speech, he said he supported changes that would allow his voting reforms to be passed without the support of opposition Republicans.\\\n",
    "Misgivings from two senators in his party are hampering his plans, and no Republicans have backed them.\\\n",
    "Currently, a majority of 60% is needed to pass most legislation in the Senate.\\\n",
    "And with the upper chamber of Congress split 50-50 between the two parties, Mr Bidens sweeping election bills are almost certain not to pass unless there is a change to that rule.\\\n",
    "Such a change is unlikely, analysts say, as it would require the support of every Democrat in the Senate as well as the tie-breaking vote of the vice-president.']\n",
    "# print(news)\n",
    "\n",
    "# 토큰화 처리\n",
    "print(news)\n",
    "news = A_token.texts_to_sequences(news)\n",
    "print('토큰화 결과:',news)\n",
    "print(len(news[0]))\n",
    "\n",
    "# 길이 처리\n",
    "news = pad_sequences(news,\n",
    "                        truncating='pre',\n",
    "                        padding = 'pre',\n",
    "                        maxlen=MY_LEN)\n",
    "print('단어 수:',len(news[0]))\n",
    "\n",
    "# 예측 결과\n",
    "pred = model.predict(news)\n",
    "print('예측 결과:',pred[0].argmax())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50558415ac93d9881e18c8c056ce7448705d901c127a568288521ecc940a9623"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
